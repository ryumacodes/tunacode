"""Textual-based REPL shell - Application entry point."""

from __future__ import annotations

import asyncio
import os
import time
from datetime import UTC, datetime
from typing import Any

from rich.console import RenderableType
from rich.markdown import Markdown
from rich.text import Text
from textual.app import App, ComposeResult
from textual.binding import Binding
from textual.containers import Container
from textual.widgets import LoadingIndicator, RichLog, Static

from tunacode.core.agents.main import process_request
from tunacode.core.constants import (
    MIN_TOOL_PANEL_LINE_WIDTH,
    RICHLOG_CLASS_PAUSED,
    RICHLOG_CLASS_STREAMING,
    TOOL_PANEL_HORIZONTAL_INSET,
    build_nextstep_theme,
    build_tunacode_theme,
)
from tunacode.core.shared_types import ModelName
from tunacode.core.state import StateManager

from tunacode.ui.renderers.agent_response import render_agent_streaming
from tunacode.ui.renderers.errors import render_exception
from tunacode.ui.renderers.panels import tool_panel_smart
from tunacode.ui.repl_support import (
    build_textual_tool_callback,
    build_tool_result_callback,
    build_tool_start_callback,
    format_user_message,
)
from tunacode.ui.shell_runner import ShellRunner
from tunacode.ui.styles import (
    STYLE_MUTED,
    STYLE_PRIMARY,
    STYLE_SUCCESS,
    STYLE_WARNING,
)
from tunacode.ui.welcome import show_welcome
from tunacode.ui.widgets import (
    CommandAutoComplete,
    Editor,
    EditorSubmitRequested,
    FileAutoComplete,
    ResourceBar,
    StatusBar,
    ToolResultDisplay,
)

# Throttle streaming display updates to reduce visual churn
STREAM_THROTTLE_MS: float = 100.0


class TextualReplApp(App[None]):
    TITLE = "TunaCode"
    CSS_PATH = [
        "styles/layout.tcss",
        "styles/widgets.tcss",
        "styles/modals.tcss",
        "styles/panels.tcss",
        "styles/theme-nextstep.tcss",
    ]

    BINDINGS = [
        Binding("ctrl+p", "toggle_pause", "Pause/Resume Stream", show=False, priority=True),
        Binding("escape", "cancel_stream", "Cancel", show=False, priority=True),
    ]

    def __init__(self, *, state_manager: StateManager, show_setup: bool = False) -> None:
        super().__init__()
        self.state_manager: StateManager = state_manager
        self._show_setup: bool = show_setup
        self.request_queue: asyncio.Queue[str] = asyncio.Queue()

        self._streaming_paused: bool = False
        self._streaming_cancelled: bool = False
        self._stream_buffer: list[str] = []
        self.current_stream_text: str = ""
        self._current_request_task: asyncio.Task | None = None
        self._loading_indicator_shown: bool = False
        self._last_display_update: float = 0.0
        self._request_start_time: float = 0.0

        self.shell_runner = ShellRunner(self)

        self.rich_log: RichLog
        self.editor: Editor
        self.resource_bar: ResourceBar
        self.status_bar: StatusBar
        self.streaming_output: Static

    def compose(self) -> ComposeResult:
        self.resource_bar = ResourceBar()
        self.rich_log = RichLog(wrap=True, markup=True, highlight=True, auto_scroll=True)
        self.streaming_output = Static("", id="streaming-output")
        self.loading_indicator = LoadingIndicator()
        self.editor = Editor()
        self.status_bar = StatusBar()

        yield self.resource_bar
        with Container(id="viewport"):
            yield self.rich_log
            yield self.loading_indicator
        yield self.streaming_output
        yield self.editor
        yield FileAutoComplete(self.editor)
        yield CommandAutoComplete(self.editor)
        yield self.status_bar

    def on_mount(self) -> None:
        tunacode_theme = build_tunacode_theme()
        self.register_theme(tunacode_theme)
        nextstep_theme = build_nextstep_theme()
        self.register_theme(nextstep_theme)

        user_config = self.state_manager.session.user_config
        saved_theme = user_config.get("settings", {}).get("theme", "dracula")
        self.theme = saved_theme if saved_theme in self.available_themes else "dracula"

        # Initialize session persistence metadata
        from tunacode.core.system_paths import get_project_id

        session = self.state_manager.session
        session.project_id = get_project_id()
        session.working_directory = os.getcwd()
        if not session.created_at:
            session.created_at = datetime.now(UTC).isoformat()

        if self._show_setup:
            from tunacode.ui.screens import SetupScreen

            self.push_screen(SetupScreen(self.state_manager), self._on_setup_complete)
        else:
            self._start_repl()

    async def on_unmount(self) -> None:
        """Save session before app exits."""
        self.state_manager.save_session()

    def watch_theme(self, old_theme: str, new_theme: str) -> None:
        """Toggle CSS class when theme changes for theme-specific styling."""
        if old_theme:
            self.remove_class(f"theme-{old_theme}")
        if new_theme:
            self.add_class(f"theme-{new_theme}")

    def _on_setup_complete(self, completed: bool) -> None:
        """Called when setup screen is dismissed."""
        if completed:
            self._update_resource_bar()
        self._start_repl()

    def _start_repl(self) -> None:
        """Initialize REPL components after setup."""
        from tunacode.core.logging import get_logger

        # Initialize logging with TUI callback
        logger = get_logger()
        logger.set_state_manager(self.state_manager)
        logger.set_tui_callback(lambda text: self.rich_log.write(text))

        self.set_focus(self.editor)
        self.run_worker(self._request_worker, exclusive=False)
        self._update_resource_bar()
        show_welcome(self.rich_log)

    async def _request_worker(self) -> None:
        while True:
            request = await self.request_queue.get()
            try:
                await self._process_request(request)
            except Exception as e:
                error_renderable = render_exception(e)
                self.rich_log.write(error_renderable)
            finally:
                self.request_queue.task_done()

    async def _process_request(self, message: str) -> None:
        self.current_stream_text = ""
        self._last_display_update = 0.0
        self._streaming_cancelled = False
        self.query_one("#viewport").add_class(RICHLOG_CLASS_STREAMING)

        self._loading_indicator_shown = True
        self.loading_indicator.add_class("active")

        self._request_start_time = time.monotonic()

        try:
            session = self.state_manager.session
            model_name = session.current_model or "openai/gpt-4o"

            self._current_request_task = asyncio.create_task(
                process_request(
                    message=message,
                    model=ModelName(model_name),
                    state_manager=self.state_manager,
                    tool_callback=build_textual_tool_callback(),
                    streaming_callback=self.streaming_callback,
                    tool_result_callback=build_tool_result_callback(self),
                    tool_start_callback=build_tool_start_callback(self),
                    notice_callback=self._show_system_notice,
                )
            )
            await self._current_request_task
        except asyncio.CancelledError:
            self.notify("Cancelled")
        except Exception as e:
            error_renderable = render_exception(e)
            self.rich_log.write(error_renderable)
        finally:
            self._current_request_task = None
            self._loading_indicator_shown = False
            self.loading_indicator.remove_class("active")
            self.query_one("#viewport").remove_class(RICHLOG_CLASS_STREAMING)
            self.query_one("#viewport").remove_class(RICHLOG_CLASS_PAUSED)
            self.streaming_output.update("")
            self.streaming_output.remove_class("active")

            if self.current_stream_text and not self._streaming_cancelled:
                from tunacode.ui.renderers.agent_response import render_agent_response

                duration_ms = (time.monotonic() - self._request_start_time) * 1000
                session = self.state_manager.session
                tokens = session.usage.last_call_usage.completion_tokens
                model = session.current_model or ""

                panel = render_agent_response(
                    content=self.current_stream_text,
                    tokens=tokens,
                    duration_ms=duration_ms,
                    model=model,
                )
                self.rich_log.write("")
                self.rich_log.write(panel, expand=True)

            self.current_stream_text = ""
            self._streaming_cancelled = False
            self._update_resource_bar()

            # Auto-save session after processing
            self.state_manager.save_session()

    async def on_editor_submit_requested(self, message: EditorSubmitRequested) -> None:
        from tunacode.ui.commands import handle_command

        if await handle_command(self, message.text):
            return

        await self.request_queue.put(message.text)

        from datetime import datetime

        timestamp = datetime.now().strftime("%I:%M %p").lstrip("0")

        self.rich_log.write("")
        render_width = max(1, self.rich_log.size.width - 2)

        user_block = format_user_message(message.text, STYLE_PRIMARY, width=render_width)

        user_block.append(f"â”‚ you {timestamp}", style=f"dim {STYLE_PRIMARY}")
        self.rich_log.write(user_block)

    def on_tool_result_display(self, message: ToolResultDisplay) -> None:
        max_line_width = self.tool_panel_max_width()
        panel = tool_panel_smart(
            name=message.tool_name,
            status=message.status,
            args=message.args,
            result=message.result,
            duration_ms=message.duration_ms,
            max_line_width=max_line_width,
        )
        self.rich_log.write(panel, expand=True)

    def tool_panel_max_width(self) -> int:
        viewport = self.query_one("#viewport")
        width_candidates = [
            self.rich_log.content_region.width,
            viewport.content_region.width,
            self.rich_log.size.width,
            viewport.size.width,
            self.size.width,
        ]
        usable_widths = [width for width in width_candidates if width > 0]
        content_width = max(usable_widths, default=MIN_TOOL_PANEL_LINE_WIDTH)
        available_width = content_width - TOOL_PANEL_HORIZONTAL_INSET
        max_line_width = max(MIN_TOOL_PANEL_LINE_WIDTH, available_width)
        return max_line_width

    def _show_system_notice(self, notice: str) -> None:
        notice_text = Text(notice, style=STYLE_WARNING)
        self.rich_log.write(notice_text)

    def _is_user_prompt_request(self, message: Any) -> bool:
        parts = getattr(message, "parts", None)
        if not parts:
            return False

        return any(getattr(part, "part_kind", None) == "user-prompt" for part in parts)

    def _replay_session_messages(self) -> None:
        """Render loaded session messages to RichLog."""
        from pydantic_ai.messages import ModelRequest, ModelResponse

        from tunacode.core.messaging import get_content

        conversation = self.state_manager.session.conversation
        for msg in conversation.messages:
            content = get_content(msg)
            if not content:
                continue

            if isinstance(msg, ModelRequest):
                if not self._is_user_prompt_request(msg):
                    continue
                user_block = Text()
                user_block.append(f"| {content}\n", style=STYLE_PRIMARY)
                user_block.append("| (restored)", style=f"dim {STYLE_PRIMARY}")
                self.rich_log.write(user_block)
            elif isinstance(msg, ModelResponse):
                self.rich_log.write(Text("agent:", style="accent"))
                self.rich_log.write(Markdown(content))

    async def streaming_callback(self, chunk: str) -> None:
        if self._streaming_paused:
            self._stream_buffer.append(chunk)
            return

        # Always accumulate immediately
        self.current_stream_text += chunk

        # Throttle display updates to reduce visual churn
        now = time.monotonic()
        elapsed_ms = (now - self._last_display_update) * 1000

        if elapsed_ms >= STREAM_THROTTLE_MS:
            self._last_display_update = now
            self._update_streaming_panel(now)
            self.streaming_output.add_class("active")
            self.rich_log.scroll_end()

    def _update_streaming_panel(self, now: float) -> None:
        """Update streaming output with current content and elapsed time."""
        elapsed_ms = (now - self._request_start_time) * 1000
        model = self.state_manager.session.current_model or ""
        panel = render_agent_streaming(self.current_stream_text, elapsed_ms, model)
        self.streaming_output.update(panel)

    def action_toggle_pause(self) -> None:
        if self._streaming_paused:
            self.resume_streaming()
        else:
            self.pause_streaming()

    def pause_streaming(self) -> None:
        self._streaming_paused = True
        self.query_one("#viewport").add_class(RICHLOG_CLASS_PAUSED)
        self.notify("Streaming paused...")

    def resume_streaming(self) -> None:
        self._streaming_paused = False
        self.query_one("#viewport").remove_class(RICHLOG_CLASS_PAUSED)
        self.notify("Streaming resumed...")

        if self._stream_buffer:
            buffered_text = "".join(self._stream_buffer)
            self.current_stream_text += buffered_text
            self._stream_buffer.clear()

        # Force immediate display update on resume
        now = time.monotonic()
        self._last_display_update = now
        self._update_streaming_panel(now)

    def action_cancel_stream(self) -> None:
        if self._current_request_task is not None:
            self._streaming_cancelled = True
            self._stream_buffer.clear()
            self.current_stream_text = ""
            self._current_request_task.cancel()
            return

        shell_runner = getattr(self, "shell_runner", None)
        if shell_runner is not None and shell_runner.is_running():
            shell_runner.cancel()
            return

        if self.editor.value or self.editor.has_paste_buffer:
            self.editor.clear_input()

    def start_shell_command(self, raw_cmd: str) -> None:
        self.shell_runner.start(raw_cmd)

    def write_shell_output(self, renderable: RenderableType) -> None:
        self.rich_log.write(renderable)

    def shell_status_running(self) -> None:
        self.status_bar.update_running_action("shell")

    def shell_status_last(self) -> None:
        self.status_bar.update_last_action("shell")

    def _update_resource_bar(self) -> None:
        session = self.state_manager.session
        conversation = session.conversation

        # Use actual context window tokens, not cumulative API usage
        context_tokens = conversation.total_tokens

        self.resource_bar.update_stats(
            model=session.current_model or "No model selected",
            tokens=context_tokens,
            max_tokens=conversation.max_tokens or 200000,
        )
